FROM apache/airflow:2.5.1-python3.8

# Copiar los archivos necesarios
COPY entrypoint.sh /opt/airflow/entrypoint.sh
COPY requirements.txt /opt/airflow/requirements.txt

# Dar permisos de ejecuci√≥n al entrypoint antes de cambiar de usuario
USER root
RUN chmod +x /opt/airflow/entrypoint.sh

# Instalar dependencias del sistema necesarias antes de cambiar de usuario
RUN apt-get update && apt-get install -y \
    openjdk-17-jdk \
    procps \
    curl \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

# Configurar variables de entorno de Java
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
ENV PATH="$JAVA_HOME/bin:$PATH"

# Descargar e instalar Spark 3.3.0
RUN curl -o /tmp/spark.tgz https://archive.apache.org/dist/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz && \
    tar -xvzf /tmp/spark.tgz -C /opt/ && \
    mv /opt/spark-3.3.0-bin-hadoop3 /opt/spark && \
    rm /tmp/spark.tgz

# Configurar variables de entorno de Spark
ENV SPARK_HOME=/opt/spark
ENV PATH="$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH"

# Configurar Airflow para usar LocalExecutor
ENV AIRFLOW__CORE__EXECUTOR=LocalExecutor

# Cambiar al usuario airflow
USER airflow

# Instalar dependencias usando el usuario airflow
RUN pip install --no-cache-dir -r /opt/airflow/requirements.txt

# Establecer el entrypoint correctamente
ENTRYPOINT ["/bin/bash", "/opt/airflow/entrypoint.sh"]
