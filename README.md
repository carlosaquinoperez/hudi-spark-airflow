# Hudi-Spark-Airflow: Procesamiento de Datos con Apache Hudi

## ğŸ“Œ Objetivo
Este proyecto implementa un flujo de procesamiento de datos utilizando **Apache Hudi, Spark y Airflow** para la ingestiÃ³n, transformaciÃ³n y almacenamiento de datos en **MinIO** (S3-compatible). Se exploran funcionalidades clave como **Upserts, Deletes, Time Travel y consultas incrementales en Hudi**.

---

## ğŸ“‚ Estructura del Proyecto

```
ğŸ“¦ hudi-spark-airflow
â”œâ”€â”€ ğŸ“‚ dags                   # DAGs de Airflow para orquestar el pipeline
â”‚   â”œâ”€â”€ hudi_pipeline.py      # DAG principal de ingestiÃ³n a Hudi
â”‚   â”œâ”€â”€ hudi_tests_pipeline.py # DAG de pruebas (upsert, delete, time travel, incremental)
â”‚
â”œâ”€â”€ ğŸ“‚ scripts                # Scripts PySpark para procesamiento de datos
â”‚   â”œâ”€â”€ convert_csv_to_parquet.py  # ConversiÃ³n de CSV a Parquet
â”‚   â”œâ”€â”€ load_parquet_to_hudi.py    # Carga de datos en Apache Hudi
â”‚   â”œâ”€â”€ upsert_hudi.py        # Prueba de UPSERT en Hudi
â”‚   â”œâ”€â”€ delete_sales_hudi.py  # Prueba de DELETE en Hudi
â”‚
â”œâ”€â”€ ğŸ“‚ config                 # Configuraciones adicionales
â”‚   â”œâ”€â”€ hive-site.xml         # ConfiguraciÃ³n de Hive Metastore
â”‚
â”œâ”€â”€ ğŸ“‚ trino                  # ConfiguraciÃ³n de Trino para consultas sobre Hudi
â”‚
â”œâ”€â”€ Dockerfile-airflow        # Imagen personalizada para Airflow
â”œâ”€â”€ Dockerfile-spark          # Imagen personalizada para Spark
â”œâ”€â”€ docker-compose.yml        # DefiniciÃ³n de los servicios Docker
â”œâ”€â”€ README.md                 # DocumentaciÃ³n del proyecto
```

---

## ğŸš€ TecnologÃ­as Utilizadas
- **Apache Hudi**: Framework para gestiÃ³n de datos incrementales en lakes
- **Apache Spark**: Motor de procesamiento distribuido
- **Apache Airflow**: Orquestador de flujos de datos
- **MinIO**: Almacenamiento de objetos S3-compatible
- **Trino**: Motor de consultas SQL distribuido
- **Hive Metastore**: CatÃ¡logo de metadatos para tablas Hudi
- **PostgreSQL**: Base de datos para Airflow y Hive Metastore
- **Docker y Docker Compose**: Contenedores para la infraestructura

---

## ğŸ”§ ConfiguraciÃ³n y EjecuciÃ³n

### 1ï¸âƒ£ Clonar el repositorio
```sh
git clone https://github.com/carlosaquinoperez/hudi-spark-airflow.git
cd hudi-spark-airflow
```

### 2ï¸âƒ£ Iniciar la infraestructura con Docker Compose
```sh
docker-compose up -d --build
```

### 3ï¸âƒ£ Acceder a los servicios
- **Airflow UI**: [http://localhost:8082](http://localhost:8082)
- **MinIO Console**: [http://localhost:9001](http://localhost:9001)
- **Trino UI**: [http://localhost:8083](http://localhost:8083)

### 4ï¸âƒ£ Cargar los DAGs en Airflow
```sh
docker exec -it airflow-webserver airflow dags list
```

### 5ï¸âƒ£ Ejecutar el pipeline en Airflow
Desde la UI de Airflow, habilitar y ejecutar los DAGs **hudi_pipeline** y **hudi_tests_pipeline**.

---

## ğŸ“Š Flujo de Datos y Arquitectura

```mermaid
graph TD;
    A[CSV Files en MinIO] -->|Spark Read| B(ConversiÃ³n a Parquet);
    B -->|Spark Write| C(Almacenamiento en MinIO);
    C -->|Spark Read| D(Carga a Apache Hudi);
    D -->|ActualizaciÃ³n en Hudi| E{Metastore & Trino};
    E -->|Consultas SQL| F(AnÃ¡lisis y VisualizaciÃ³n en Trino);
```

---

## ğŸ§ª Pruebas Realizadas

### âœ… 1. UPSERT en Apache Hudi
**Objetivo**: Insertar nuevos registros y actualizar existentes.
```sh
docker exec -it spark-master spark-submit /opt/airflow/scripts/upsert_hudi.py
```

### âœ… 2. DELETE en Apache Hudi
**Objetivo**: Eliminar registros especÃ­ficos en la tabla Hudi.
```sh
docker exec -it spark-master spark-submit /opt/airflow/scripts/delete_sales_hudi.py
```

### âœ… 3. Time Travel en Apache Hudi
**Objetivo**: Consultar versiones anteriores de los datos.
```sql
SELECT * FROM hudi.default.walmart_sales_hudi FOR SYSTEM_TIME AS OF '20250304104111539';
```

### âœ… 4. Consultas Incrementales
**Objetivo**: Obtener solo los registros modificados en el Ãºltimo commit.
```sql
SELECT * FROM hudi.default.walmart_sales_hudi WHERE _hoodie_commit_time > '20250304104111539';
```

---

## ğŸ“Œ Conclusiones
Este proyecto demuestra cÃ³mo implementar un pipeline de datos eficiente utilizando Apache Hudi para gestiÃ³n de datos incrementales, Spark para el procesamiento distribuido y Airflow para la orquestaciÃ³n. AdemÃ¡s, se exploraron funcionalidades avanzadas como **Time Travel y consultas incrementales**.

### ğŸš€ PrÃ³ximos Pasos
- Implementar **clustering** y **compaction** en Hudi
- Optimizar el rendimiento con **Bloom Filters** y **Indexing**
- Integrar con **Apache Kafka** para streaming en tiempo real

